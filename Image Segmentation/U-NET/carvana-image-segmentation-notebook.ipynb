{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-25T17:57:30.230432Z","iopub.execute_input":"2023-10-25T17:57:30.230731Z","iopub.status.idle":"2023-10-25T17:57:30.592252Z","shell.execute_reply.started":"2023-10-25T17:57:30.230704Z","shell.execute_reply":"2023-10-25T17:57:30.591347Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/carvana-image-masking-challenge/train_masks.zip\n/kaggle/input/carvana-image-masking-challenge/29bb3ece3180_11.jpg\n/kaggle/input/carvana-image-masking-challenge/train_masks.csv.zip\n/kaggle/input/carvana-image-masking-challenge/train.zip\n/kaggle/input/carvana-image-masking-challenge/metadata.csv.zip\n/kaggle/input/carvana-image-masking-challenge/sample_submission.csv.zip\n/kaggle/input/carvana-image-masking-challenge/test.zip\n/kaggle/input/carvana-image-masking-challenge/test_hq.zip\n/kaggle/input/carvana-image-masking-challenge/train_hq.zip\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:30.594164Z","iopub.execute_input":"2023-10-25T17:57:30.594662Z","iopub.status.idle":"2023-10-25T17:57:30.599206Z","shell.execute_reply.started":"2023-10-25T17:57:30.594625Z","shell.execute_reply":"2023-10-25T17:57:30.598311Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# importing the zipfile module \nfrom zipfile import ZipFile \n\nwith ZipFile(\"/kaggle/input/carvana-image-masking-challenge/train.zip\", 'r') as f:  \n    f.extractall( path=\"/kaggle/working/train_images/\") \n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:30.600503Z","iopub.execute_input":"2023-10-25T17:57:30.600830Z","iopub.status.idle":"2023-10-25T17:57:38.323937Z","shell.execute_reply.started":"2023-10-25T17:57:30.600799Z","shell.execute_reply":"2023-10-25T17:57:38.322975Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"with ZipFile(\"/kaggle/input/carvana-image-masking-challenge/train_masks.zip\", 'r') as f:  \n    f.extractall( path=\"/kaggle/working/\") ","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:38.326007Z","iopub.execute_input":"2023-10-25T17:57:38.326286Z","iopub.status.idle":"2023-10-25T17:57:39.198965Z","shell.execute_reply.started":"2023-10-25T17:57:38.326261Z","shell.execute_reply":"2023-10-25T17:57:39.198151Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_imgs = sorted(os.listdir(\"/kaggle/working/train_images/train\"))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-25T17:57:39.199972Z","iopub.execute_input":"2023-10-25T17:57:39.200216Z","iopub.status.idle":"2023-10-25T17:57:39.210047Z","shell.execute_reply.started":"2023-10-25T17:57:39.200193Z","shell.execute_reply":"2023-10-25T17:57:39.209203Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_masks = sorted(os.listdir(\"/kaggle/working/train_masks\"))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-10-25T17:57:39.211272Z","iopub.execute_input":"2023-10-25T17:57:39.211585Z","iopub.status.idle":"2023-10-25T17:57:39.222704Z","shell.execute_reply.started":"2023-10-25T17:57:39.211562Z","shell.execute_reply":"2023-10-25T17:57:39.221939Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_imgs[:16], train_masks[:16]","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:39.223804Z","iopub.execute_input":"2023-10-25T17:57:39.224193Z","iopub.status.idle":"2023-10-25T17:57:39.232719Z","shell.execute_reply.started":"2023-10-25T17:57:39.224162Z","shell.execute_reply":"2023-10-25T17:57:39.231860Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(['00087a6bd4dc_01.jpg',\n  '00087a6bd4dc_02.jpg',\n  '00087a6bd4dc_03.jpg',\n  '00087a6bd4dc_04.jpg',\n  '00087a6bd4dc_05.jpg',\n  '00087a6bd4dc_06.jpg',\n  '00087a6bd4dc_07.jpg',\n  '00087a6bd4dc_08.jpg',\n  '00087a6bd4dc_09.jpg',\n  '00087a6bd4dc_10.jpg',\n  '00087a6bd4dc_11.jpg',\n  '00087a6bd4dc_12.jpg',\n  '00087a6bd4dc_13.jpg',\n  '00087a6bd4dc_14.jpg',\n  '00087a6bd4dc_15.jpg',\n  '00087a6bd4dc_16.jpg'],\n ['00087a6bd4dc_01_mask.gif',\n  '00087a6bd4dc_02_mask.gif',\n  '00087a6bd4dc_03_mask.gif',\n  '00087a6bd4dc_04_mask.gif',\n  '00087a6bd4dc_05_mask.gif',\n  '00087a6bd4dc_06_mask.gif',\n  '00087a6bd4dc_07_mask.gif',\n  '00087a6bd4dc_08_mask.gif',\n  '00087a6bd4dc_09_mask.gif',\n  '00087a6bd4dc_10_mask.gif',\n  '00087a6bd4dc_11_mask.gif',\n  '00087a6bd4dc_12_mask.gif',\n  '00087a6bd4dc_13_mask.gif',\n  '00087a6bd4dc_14_mask.gif',\n  '00087a6bd4dc_15_mask.gif',\n  '00087a6bd4dc_16_mask.gif'])"},"metadata":{}}]},{"cell_type":"code","source":"# 5088 images, let us keep 80 (5x16) images as validation set","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:39.233751Z","iopub.execute_input":"2023-10-25T17:57:39.234054Z","iopub.status.idle":"2023-10-25T17:57:39.243334Z","shell.execute_reply.started":"2023-10-25T17:57:39.234031Z","shell.execute_reply":"2023-10-25T17:57:39.242627Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/val_images\")\nos.makedirs(\"/kaggle/working/val_masks\")","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:39.244290Z","iopub.execute_input":"2023-10-25T17:57:39.244591Z","iopub.status.idle":"2023-10-25T17:57:39.254388Z","shell.execute_reply.started":"2023-10-25T17:57:39.244568Z","shell.execute_reply":"2023-10-25T17:57:39.253677Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import shutil","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:39.257091Z","iopub.execute_input":"2023-10-25T17:57:39.257378Z","iopub.status.idle":"2023-10-25T17:57:39.266108Z","shell.execute_reply.started":"2023-10-25T17:57:39.257355Z","shell.execute_reply":"2023-10-25T17:57:39.265347Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for idx, (image, mask) in enumerate(zip(train_imgs, train_masks)):\n    if idx == 80:\n        break\n    \n    img_path = os.path.join(\"/kaggle/working/train_images/train/\", image)\n    mask_path = os.path.join(\"/kaggle/working/train_masks/\", mask)\n    \n    new_img_path = os.path.join(\"/kaggle/working/val_images/\", image)\n    new_mask_path = os.path.join(\"/kaggle/working/val_masks/\", mask)\n\n    shutil.move(img_path, new_img_path)\n    shutil.move(mask_path, new_mask_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:39.267097Z","iopub.execute_input":"2023-10-25T17:57:39.267472Z","iopub.status.idle":"2023-10-25T17:57:39.281770Z","shell.execute_reply.started":"2023-10-25T17:57:39.267414Z","shell.execute_reply":"2023-10-25T17:57:39.281067Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"len(os.listdir(\"/kaggle/working/train_images/train\")), len(os.listdir(\"/kaggle/working/train_masks\"))","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:39.282812Z","iopub.execute_input":"2023-10-25T17:57:39.283084Z","iopub.status.idle":"2023-10-25T17:57:39.300786Z","shell.execute_reply.started":"2023-10-25T17:57:39.283061Z","shell.execute_reply":"2023-10-25T17:57:39.299889Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(5008, 5008)"},"metadata":{}}]},{"cell_type":"code","source":"len(os.listdir(\"/kaggle/working/val_images\")), len(os.listdir(\"/kaggle/working/val_masks\"))","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:39.302276Z","iopub.execute_input":"2023-10-25T17:57:39.302612Z","iopub.status.idle":"2023-10-25T17:57:39.309564Z","shell.execute_reply.started":"2023-10-25T17:57:39.302583Z","shell.execute_reply":"2023-10-25T17:57:39.308586Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(80, 80)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\n\n\nclass DoubleConv(nn.Module):  # (conv 3x3, ReLU)x2 blocks\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNET(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, out_channels_list=None):\n        if out_channels_list is None:\n            out_channels_list = [64, 128, 256, 512]\n\n        super(UNET, self).__init__()\n\n        self.downs = nn.ModuleList()  # storing list of modules\n        self.ups = nn.ModuleList()\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # UNET down sampling (4 blocks)\n        for out_channel_size in out_channels_list:\n            self.downs.append(DoubleConv(in_channels, out_channel_size))\n            in_channels = out_channel_size  # update in_channels\n\n        # Bottleneck layer (lowermost)\n        self.bottleneck = DoubleConv(in_channels=out_channels_list[-1],\n                                     out_channels=out_channels_list[-1]*2)\n\n        # UNET up sampling (4 blocks)\n        for out_channel_size in reversed(out_channels_list):\n            self.ups.append(  # in_channels=out_channel_size*2 because of channels concat from skip connection\n                nn.ConvTranspose2d(out_channel_size*2, out_channel_size, kernel_size=2, stride=2)\n            )  # green arrows (up-conv 2x2)\n            self.ups.append(DoubleConv(out_channel_size*2, out_channel_size))  # (conv 3x3, ReLU)x2\n\n        # Final convolution (channel reduction conv)\n        self.final_conv = nn.Conv2d(in_channels=out_channels_list[0],\n                                    out_channels=out_channels,\n                                    kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n\n        # creating red arrows (max pool 2x2) [down sampling]\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        # bottleneck layer\n        x = self.bottleneck(x)\n        # we now need the values in reverse while going in up sample part\n        skip_connections.reverse()\n\n        # creating green arrows [up sampling]\n        for i in range(0, len(self.ups), 2):  # step of 2 as we want only the conv\n            x = self.ups[i](x)\n            skip_connection = skip_connections[i//2]  # because our loop is running with step 2\n\n            if x.shape != skip_connection.shape:  # if w, h not a multiple of 16, there is flooring of values\n                x = TF.resize(x, size=skip_connection.shape[2:])  # resize just h and w of the feature map\n\n            concatenation = torch.cat([skip_connection, x], dim=1)  # channel concatenation\n            x = self.ups[i+1](concatenation)\n\n        # channel reduction convolutions\n        x = self.final_conv(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:39.310887Z","iopub.execute_input":"2023-10-25T17:57:39.311414Z","iopub.status.idle":"2023-10-25T17:57:42.958415Z","shell.execute_reply.started":"2023-10-25T17:57:39.311381Z","shell.execute_reply":"2023-10-25T17:57:42.957593Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport numpy as np\n\n\nclass CarvanaDataset(Dataset):\n    \"\"\"\n    creating the carvana dataset for our model\n    \"\"\"\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = os.listdir(image_dir)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, item):\n        img_path = os.path.join(self.image_dir, self.images[item])\n        mask_path = os.path.join(self.mask_dir, self.images[item].replace('.jpg', '_mask.gif'))\n        # reading the image and mask\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n        mask[mask == 255.0] = 1.0  # setting all bright pixels to 1\n        # 1.0, as a positive label, as later on we'll use sigmoid function\n\n        if self.transform is not None:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:42.959573Z","iopub.execute_input":"2023-10-25T17:57:42.959956Z","iopub.status.idle":"2023-10-25T17:57:42.969574Z","shell.execute_reply.started":"2023-10-25T17:57:42.959931Z","shell.execute_reply":"2023-10-25T17:57:42.968377Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Utils","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/saved_images/\")","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:57:42.971234Z","iopub.execute_input":"2023-10-25T17:57:42.971565Z","iopub.status.idle":"2023-10-25T17:57:42.983324Z","shell.execute_reply.started":"2023-10-25T17:57:42.971540Z","shell.execute_reply":"2023-10-25T17:57:42.982618Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torch.utils.data import DataLoader\n\n\ndef save_checkpoint(state, epoch, filename=\"/kaggle/working/checkpoint.pth\"):\n    print(\"=> Saving checkpoint\")\n    filename = f\"/kaggle/working/checkpoint{epoch}.pth\"\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\n\ndef get_data_loaders(train_dir, train_maskdir, val_dir, val_maskdir, batch_size, train_transform, val_transform, num_workers=4, pin_memory=True):\n    train_ds = CarvanaDataset(\n        image_dir=train_dir,\n        mask_dir=train_maskdir,\n        transform=train_transform,\n    )\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=True,\n    )\n\n    val_ds = CarvanaDataset(\n        image_dir=val_dir,\n        mask_dir=val_maskdir,\n        transform=val_transform,\n    )\n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=False,\n    )\n\n    return train_loader, val_loader\n\n\ndef check_accuracy(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n            # dice score = 2 * size of the intersection / sum of the sizes of the two sets\n            # similarity between two samples (here, segmentation maps which are images)\n\n    print(f\"Got {num_correct}/{num_pixels} with acc {num_correct / num_pixels * 100:.2f}\")\n    print(f\"Dice score: {dice_score / len(loader)}\")\n    model.train()\n\n\ndef save_predictions_as_imgs(loader, model, folder=\"/kaggle/working/saved_images/\", device=\"cuda\"):\n    model.eval()  # set model to evaluation mode\n    for idx, (x, y) in enumerate(loader):\n        x = x.to(device=device)\n        with torch.no_grad():\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n        torchvision.utils.save_image(\n            preds, f\"{folder}/pred_{idx}.png\"\n        )\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n\n    model.train()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T17:58:36.517089Z","iopub.execute_input":"2023-10-25T17:58:36.517974Z","iopub.status.idle":"2023-10-25T17:58:36.536683Z","shell.execute_reply.started":"2023-10-25T17:58:36.517934Z","shell.execute_reply":"2023-10-25T17:58:36.535673Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Training\n","metadata":{}},{"cell_type":"code","source":"# Imports\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n# Hyperparameters\nLEARNING_RATE = 1e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\nNUM_EPOCHS = 5\nNUM_WORKERS = 2\nIMAGE_HEIGHT = 320  # 1280 originally\nIMAGE_WIDTH = 480  # 1918 originally\nPIN_MEMORY = True\nLOAD_MODEL = False\nTRAIN_IMG_DIR = \"/kaggle/working/train_images/train/\"\nTRAIN_MASK_DIR = \"/kaggle/working/train_masks/\"\nVAL_IMG_DIR = \"/kaggle/working/val_images\"\nVAL_MASK_DIR = \"/kaggle/working/val_masks\"\n\n\ndef train_loop(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader)\n\n    for batch_idx, (X, y) in enumerate(loop):\n        X = X.to(device=DEVICE)\n        y = y.float().unsqueeze(1).to(device=DEVICE)  # .unsqueeze(1) to add as a channel dimension\n\n        # forward pass (incorporating mixed precision training)\n        with torch.cuda.amp.autocast():\n            # mixed precision (float16 sometimes for quicker training, while maintaining accuracy)\n            predictions = model(X)\n            loss = loss_fn(predictions, y)\n\n        # backward pass\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # update tqdm loop\n        loop.set_postfix(loss=loss.item())\n\n\n#####################\n### Main Function ###\n#####################\n\ntrain_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=30, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,  # pixel val between 0 to 1\n        ),\n        ToTensorV2(),\n    ],\n)\n\nval_transforms = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],\n)\n\nmodel = UNET(in_channels=3, out_channels=1).to(DEVICE)\nloss_fn = nn.BCEWithLogitsLoss()  # similar to from_logits = True (tensorflow)\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\ntrain_loader, val_loader = get_data_loaders(\n    TRAIN_IMG_DIR,\n    TRAIN_MASK_DIR,\n    VAL_IMG_DIR,\n    VAL_MASK_DIR,\n    BATCH_SIZE,\n    train_transform,\n    val_transforms,\n    NUM_WORKERS,\n    PIN_MEMORY,\n)\n\nif LOAD_MODEL:\n    try:\n        load_checkpoint(torch.load(\"/kaggle/working/checkpoint.pth\"), model)\n    except EOFError:  # end of file error - if no such file found, do nothing (pass)\n        pass\n\n# check_accuracy(val_loader, model, device=DEVICE)  # check accuracy after loading a checkpoint\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(NUM_EPOCHS):\n    train_loop(train_loader, model, optimizer, loss_fn, scaler)\n\n    # save model\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint, epoch)\n\n    # check accuracy\n    check_accuracy(val_loader, model, device=DEVICE)\n\n    # print some examples to a folder\n    save_predictions_as_imgs(\n        val_loader, model, folder=\"/kaggle/working/saved_images/\", device=DEVICE\n    )","metadata":{"execution":{"iopub.status.busy":"2023-10-25T18:37:16.346384Z","iopub.execute_input":"2023-10-25T18:37:16.347097Z","iopub.status.idle":"2023-10-25T19:01:50.247590Z","shell.execute_reply.started":"2023-10-25T18:37:16.347065Z","shell.execute_reply":"2023-10-25T19:01:50.246253Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"100%|██████████| 313/313 [04:46<00:00,  1.09it/s, loss=0.147]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 12098644/12288000 with acc 98.46\nDice score: 0.9642143249511719\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 313/313 [04:46<00:00,  1.09it/s, loss=0.0874]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 12224047/12288000 with acc 99.48\nDice score: 0.9874655604362488\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 313/313 [04:46<00:00,  1.09it/s, loss=0.058] \n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 12215935/12288000 with acc 99.41\nDice score: 0.9859868288040161\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 313/313 [04:46<00:00,  1.09it/s, loss=0.0402]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 12242185/12288000 with acc 99.63\nDice score: 0.9910537600517273\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 313/313 [04:46<00:00,  1.09it/s, loss=0.0302]\n","output_type":"stream"},{"name":"stdout","text":"=> Saving checkpoint\nGot 12174267/12288000 with acc 99.07\nDice score: 0.9781190752983093\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}